{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def load_image(image_path, device, output_size=None):\n",
    "    \"\"\"Loads an image by transforming it into a tensor.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    output_dimensions = None\n",
    "    if output_size is None:\n",
    "        output_dimensions = (img.size[1], img.size[0]) #if there is no output size specified, we use the content image size\n",
    "    elif isinstance(output_size, tuple):\n",
    "        if (len(output_size) == 2) and isinstance(output_size[0], int) and isinstance(output_size[1], int): #checking if the height and width are both an int if they are specified as a tuple\n",
    "            output_dimensions = output_size\n",
    "    elif isinstance(output_size, int):\n",
    "        output_dimensions = (output_size, output_size) #if only one value is provided, we make a square image\n",
    "    else:\n",
    "        raise ValueError(\"Provide dimension value as int, or tuple of int in format (height, width).\")\n",
    "\n",
    "    torch_load = transforms.Compose([transforms.Resize(output_dimensions),transforms.ToTensor()])\n",
    "    \n",
    "    return torch_load(img).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision.models import vgg19\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "class ImageStyleTransfer_VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageStyleTransfer_VGG19, self).__init__()\n",
    "\n",
    "        self.chosen_features = {0: 'conv11', 5: 'conv21', 10: 'conv31', 19: 'conv41', 28: 'conv51'}\n",
    "        self.model = vgg19(weights='DEFAULT').features[:29]\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_maps = dict()\n",
    "        for i, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "            if i in self.chosen_features.keys():\n",
    "                feature_maps[self.chosen_features[i]] = x\n",
    "        \n",
    "        return feature_maps\n",
    "\n",
    "\n",
    "def _get_content_loss(content_feature, generated_feature):\n",
    "    \"\"\"Compute MSE between content feature map and generated feature map as content loss.\"\"\"\n",
    "    return torch.mean(np.square(generated_feature - content_feature))\n",
    "\n",
    "\n",
    "def _get_style_loss(style_feature, generated_feature):\n",
    "    \"\"\"Compute MSE between gram matrix of style feature map and of generated feature map as style loss.\"\"\"\n",
    "    _, channel, height, width = generated_feature.shape\n",
    "    style_gram = style_feature.view(channel, height*width).mm(style_feature.view(channel, height*width).t())\n",
    "    generated_gram = generated_feature.view(channel, height*width).mm(generated_feature.view(channel, height*width).t())\n",
    "\n",
    "    return torch.mean(np.square(generated_gram - style_gram))\n",
    "\n",
    "\n",
    "def train_image(content, style, generated, device, train_config, output_dir, output_img_fmt, content_img_name, style_img_name, verbose=False):\n",
    "    \"\"\"Update the output image using pre-trained VGG19 model.\"\"\"\n",
    "    model = ImageStyleTransfer_VGG19().to(device).eval()    # freeze parameters in the model\n",
    "\n",
    "    # set default value for each configuration if not specified in train_config\n",
    "    num_epochs = train_config.get('num_epochs') if train_config.get('num_epochs') is not None else 6000\n",
    "    lr = train_config.get('learning_rate') if train_config.get('learning_rate') is not None else 0.001\n",
    "    alpha = train_config.get('alpha') if train_config.get('alpha') is not None else 1\n",
    "    beta = train_config.get('beta') if train_config.get('beta') is not None else 0.01\n",
    "    capture_content_features_from = train_config.get('capture_content_features_from') \\\n",
    "        if train_config.get('capture_content_features_from') is not None else {'conv11', 'conv21', 'conv31', 'conv41', 'conv51'}\n",
    "    capture_style_features_from = train_config.get('capture_style_features_from') \\\n",
    "        if train_config.get('capture_style_features_from') is not None else {'conv11', 'conv21', 'conv31', 'conv41', 'conv51'}\n",
    "            \n",
    "    # check if values passed to capture_content_features_from and capture_style_features_from are valid\n",
    "    if not isinstance(capture_content_features_from, set):\n",
    "        if isinstance(capture_content_features_from, str):\n",
    "            capture_content_features_from = set([item.strip() for item in capture_content_features_from.split(',')])\n",
    "        elif isinstance(capture_content_features_from, dict):\n",
    "            capture_content_features_from = set(capture_content_features_from.keys())\n",
    "        else:\n",
    "            print(f\"Invalid Capture Content Features\")\n",
    "            return 0\n",
    "        \n",
    "    if not capture_content_features_from.issubset({'conv11', 'conv21', 'conv31', 'conv41', 'conv51'}):\n",
    "        print(f\"Invalid Capture Content Features\")\n",
    "        return 0\n",
    "    \n",
    "    if not isinstance(capture_style_features_from, set):\n",
    "        if isinstance(capture_style_features_from, dict):\n",
    "            capture_style_features_from = set(capture_style_features_from.keys())\n",
    "        elif isinstance(capture_style_features_from, str):\n",
    "            capture_style_features_from = set([item.strip() for item in capture_style_features_from.split(',')])\n",
    "        else:\n",
    "            print(f\"Invalid Capture Content Features\")\n",
    "            return 0\n",
    "        \n",
    "    if not capture_style_features_from.issubset({'conv11', 'conv21', 'conv31', 'conv41', 'conv51'}):\n",
    "        print(f\"Invalid Capture Content Features\")\n",
    "        return 0\n",
    "\n",
    "    optimizer = torch.optim.Adam([generated], lr=lr)\n",
    "\n",
    "    if verbose:\n",
    "        # create a directory to save intermediate results\n",
    "        intermediate_dir = os.path.join(output_dir, f'nst-{content_img_name}-{style_img_name}-intermediate')\n",
    "        if not os.path.exists(intermediate_dir):\n",
    "            os.makedirs(intermediate_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # get features maps of content, style and generated images from chosen layers\n",
    "        content_features = model(content)\n",
    "        style_features = model(style)\n",
    "        generated_features = model(generated)\n",
    "\n",
    "        content_loss = style_loss = 0\n",
    "\n",
    "        for layer_name in generated_features.keys():\n",
    "            content_feature = content_features[layer_name]\n",
    "            style_feature = style_features[layer_name]\n",
    "            generated_feature = generated_features[layer_name]\n",
    "\n",
    "            if layer_name in capture_content_features_from:\n",
    "                content_loss_per_feature = _get_content_loss(content_feature, generated_feature)\n",
    "                content_loss += content_loss_per_feature\n",
    "            \n",
    "            if layer_name in capture_style_features_from:\n",
    "                style_loss_per_feature = _get_style_loss(style_feature, generated_feature)\n",
    "                style_loss += style_loss_per_feature\n",
    "\n",
    "        # compute loss \n",
    "        total_loss = alpha * content_loss + beta * style_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print loss value and save progress every 100 epochs\n",
    "        if verbose:\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                save_image(generated, os.path.join(intermediate_dir, f'nst-{content_img_name}-{style_img_name}-{epoch + 1}.{output_img_fmt}'))\n",
    "\n",
    "                print(f\"\\tEpoch {epoch + 1}/{num_epochs}, loss = {total_loss.item()}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\t================================\")\n",
    "        print(f\"\\tIntermediate images are saved in directory: '{intermediate_dir}'\")\n",
    "        print(\"\\t================================\")\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import PIL\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def image_style_transfer(config):\n",
    "    \"\"\"Implements neural style transfer on a content image using a style image, applying provided configuration.\"\"\"\n",
    "    if config.get('image_dir') is not None:\n",
    "        image_dir = config.get('image_dir')\n",
    "        content_path = os.path.join(image_dir, config.get('content_filename'))\n",
    "        style_path = os.path.join(image_dir, config.get('style_filename'))\n",
    "        output_dir = config.get('output_dir') if config.get('output_dir') is not None else image_dir\n",
    "    else:\n",
    "        output_dir = config.get('output_dir')\n",
    "        content_path = config.get('content_filepath')\n",
    "        style_path = config.get('style_path')\n",
    "\n",
    "\n",
    "    try:\n",
    "        content_img = Image.open(content_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: could not find such file: '{content_path}'.\")\n",
    "        return\n",
    "    except PIL.UnidentifiedImageError:\n",
    "        print(f\"ERROR: could not identify image file: '{content_path}'.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        style_img = Image.open(style_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: could not find such file: '{style_path}'.\")\n",
    "        return\n",
    "    except PIL.UnidentifiedImageError:\n",
    "        print(f\"ERROR: could not identify image file: '{style_path}'.\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # load content and style images\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output_size = config.get('output_image_size')\n",
    "    if output_size is not None:\n",
    "        if len(output_size) > 1: \n",
    "            output_size = tuple(output_size)\n",
    "        else:\n",
    "            output_size = output_size[0]\n",
    "\n",
    "    content_tensor = load_image(content_path, device, output_size=output_size)\n",
    "    output_size = (content_tensor.shape[2], content_tensor.shape[3])\n",
    "    style_tensor = load_image(style_path, device, output_size=output_size)\n",
    "\n",
    "\n",
    "    # initialize output image\n",
    "    generated_tensor = content_tensor.clone().requires_grad_(True)\n",
    "\n",
    "\n",
    "    # load training configuration if provided\n",
    "    train_config = dict()\n",
    "    if (train_config_path := config.get('train_config_path')) is not None:\n",
    "        try:\n",
    "            with open(train_config_path, 'r') as f:\n",
    "                train_config = yaml.safe_load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: could not find such file: '{train_config_path}'.\")\n",
    "            return\n",
    "        except yaml.YAMLError:\n",
    "            print(f\"ERROR: fail to load yaml file: '{train_config_path}'.\")\n",
    "            return\n",
    "\n",
    "        print(\"Training configuration file successfully loaded.\")\n",
    "        print()\n",
    "        \n",
    "    print(\"Training Model Now: \")\n",
    "    \n",
    "    content_img_name, content_img_fmt = os.path.splitext(os.path.basename(content_path))[0], os.path.splitext(os.path.basename(content_path))[1][1:]\n",
    "    style_img_name, style_img_fmt = os.path.splitext(os.path.basename(style_path))[0], os.path.splitext(os.path.basename(style_path))[1][1:]\n",
    "\n",
    "    output_img_fmt = config.get('output_image_format')\n",
    "    if output_img_fmt == 'same':\n",
    "        output_img_fmt = content_img_fmt\n",
    "\n",
    "    # train model\n",
    "    success = train_image(content_tensor, style_tensor, generated_tensor, device, train_config, output_dir, output_img_fmt, content_img_name, style_img_name, verbose=False)\n",
    "\n",
    "    # save output image to specified directory\n",
    "    if success:\n",
    "        save_image(generated_tensor, os.path.join(output_dir, f'nst-{content_img_name}-{style_img_name}-final.{output_img_fmt}'))\n",
    "        print(f\"Output image successfully generated as {os.path.join(output_dir, f'nst-{content_img_name}-{style_img_name}-final.{output_img_fmt}')}.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Entry point of the program.\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--image_dir\", type=str, help=\"Path to the directory where content image and style image are stored.\")\n",
    "    parser.add_argument(\"--content_filename\", type=str, default=\"content.jpg\", help=\"File name of the content image in image_dir. Will use \\\"content.jpg\\\" if not provided.\")\n",
    "    parser.add_argument(\"--style_filename\", type=str, default=\"style.jpg\", help=\"File name of the style image in image_dir. Will use \\\"style.jpg\\\" if not provided.\")\n",
    "    parser.add_argument(\"--content_filepath\", required=\"--image_dir\" not in sys.argv, type=str, help=\"Path to the content image if image_dir not provided.\")\n",
    "    parser.add_argument(\"--style_filepath\", required=\"--image_dir\" not in sys.argv, type=str, help=\"Path to the style image if image_dir not provided.\")\n",
    "    parser.add_argument(\"--output_dir\", required=\"--image_dir\" not in sys.argv, type=str, help=\"Directory that stores the output image. Will be the same as image_dir if not provided while image_dir provided.\")\n",
    "    parser.add_argument(\"--output_image_size\", nargs=\"+\", type=int, help=\"Size of the output image. Either one integer or two integers separated by space is accepted. Will use the dimensions of content image if not provided.\")\n",
    "    parser.add_argument(\"--output_image_format\", choices=[\"jpg\", \"png\", \"jpeg\", \"same\"], default=\"jpg\", help=\"Format of the output image. Can be either \\\"jpg\\\", \\\"png\\\", \\\"jpeg\\\", or \\\"same\\\". If \\\"same\\\", output image will have the same format as the content image. \\\"jpg\\\" will be the default format.\")\n",
    "    parser.add_argument(\"--train_config_path\", type=str, help=\"Path to training configuration file in .yaml format. May include: num_epochs, learning_rate, alpha, beta, capture_content_features_from, capture_style_features_from.\")\n",
    "    parser.add_argument(\"--quiet\", type=bool, default=False, help=\"True stops showing debugging messages, loss function values during training process, and stops generating intermediate images.\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    config = dict()\n",
    "    for arg in vars(args):\n",
    "        config[arg] = getattr(args, arg)\n",
    "    \n",
    "    image_style_transfer(config)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
